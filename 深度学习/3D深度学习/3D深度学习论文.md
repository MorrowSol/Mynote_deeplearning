# 目录

| 网络                                           | 时间 | 期刊 | 方式       | 类型            |
| ---------------------------------------------- | ---- | ---- | ---------- | --------------- |
| [PointNet](# PointNet)                         | 2017 | CVPR | 基于点云   | 3D分类/语义分割 |
| [PointNet++](# PointNet++)                     | 2017 | NIPS | 基于点云   | 3D分类/语义分割 |
| [F-PointNet](# Frustum PointNets (F-PointNet)) | 2018 | CVPR | 单目+点云  | 3D目标检测      |
| [VoxelNet](# VoxelNet)                         | 2018 | CVPR | 点云转体素 | 3D目标检测      |
| [PointPillars](# VoxelNet)                     | 2019 | CVPR | 点云转伪2D | 3D目标检测      |
| [Votenet](# Votenet)                           | 2019 | ICCV | 基于点云   | 3D目标检测      |
| [DSGN](# DSGN)                                 | 2020 | CVPR | 双目       | 3D目标检测      |
| [MonoPair](# MonoPair)                         | 2020 | CVPR | 单目       | 3D目标检测      |
|                                                |      |      |            |                 |

# 论文

## PointNet

> PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation

***直接使用点云作为输入，点云具有无序型，但实际点云的顺序对于物体识别是有关的，网络需要学习这种关系***

### 本文主要贡献

- 网络直接使用点云作为输入
- 展示了如何训练这样的网络
- 对该方法的稳定性和效率作了理论分析
- 可视化网络学习的特征

从数据结构的角度来看，点云是一个无序的向量集。大多数深度学习网络都在学习常规的输入表示，如序列（语音和语言处理）、图像和体积（视频或三维数据），但在点集的深度学习上并没有作太多的工作。

### 问题定义

点云：n个点 每个点（x, y, z） N* 3   共k类

分类任务：从场景中分割出某个物体区域  该区域的点云作为输入  -》 输出类别 k

分割任务：该场景点云作为输入 -》输出每个的类别 N* k

### 点云输入特性

- 无序性
- 点之间存在相互关系
- 变换下点云特性不变（如经过旋转和平移后，全局点云类别不变，分割语义不变）

### 网络架构

<img src="3D深度学习论文.assets/image-20220711135311626.png" alt="image-20220711135311626"  />

three key modules:

- the max pooling layer as a symmetric function to aggregate information from all the points

- a local and global information combination structure

- two joint alignment networks that align both input points and point features.

**三个关键模块：**

- 作为对称函数的最大池化层用来聚合点云信息 

- 一个局部全局关系组合结构

- 两个联合对齐网络用来对齐输入的点和点的特征

#### 1. 无序输入的对称函数

为了使模型对输入的变化不产生影响，有三种策略

- 将输入按一定顺序排序，需有一定的顺序，但找不到一个合适的排序方式
- 将输入作为一个序列训练RNN，通过各种排列组合增强数据，使输出对输入顺序不敏感，但顺序很重要不能忽略
- 使用一个简单的对称函数来汇总每个点的信息，这里将n个向量作为输入，输出一个对输入位置不变的新向量例如，+和∗运算符是对称的二进制函数。

作者选择第三种方式，通过对集合中的转换元素应用对称函数来近似定义在一个点集合上的一般函数
$$
f({x_1,...,x_n})\approx g(h(x_1),...,h(x_n))
$$
where $f : 2^{\mathbb{R}^N} → \mathbb{R}, h : \mathbb{R}^N → \mathbb{R}^K $and $g :\underbrace{\mathbb{R}^{K} \times \cdots \times \mathbb{R}^{K}}_{n} \rightarrow \mathbb{R} $ is a symmetric function.

从经验上看，我们的基本模块非常简单：我们用一个多层感知器网络来近似h，用一个单变量函数和一个最大集合函数的组合来近似g。实验发现这样做效果很好。通过一个h的集合，我们可以学习一些f来捕捉集合的不同属性。

#### 2. 局部全局信息聚合

直接拼接见图

#### 3. 联合对齐网络

为了使点云经过某种变换对语义标签不变，一个普遍的解决方案是将输入集对齐到一个典型的空间***（如二维深度学习中的resize、归一化）***

我们通过一个预测一个变换矩阵，并将这个变换应用与输入点的坐标***（类似于一个直角坐标系里的圆变换到极坐标系，或是一个矩形通过乘以一个矩阵，实现这个矩形的放缩，旋转，平移）***

然而特征空间的变换矩阵比空间变换矩阵的维度高得多，这大大增加了优化的难度，所以我们在损失中加入了一个正则化项，将特征变换矩阵约束为接近正交矩阵
$$
L_{reg}=||I-AA^T||_F^2
$$
A是预测的变换矩阵，正交变换不会丢失输入的信息***（在空间变换中正交变换不改变物体形状）***

### 理论分析

#### 通用近似

直觉上讲，对输入集的扰动不改变函数值，如分类或分割结果

Formally, let  $\mathcal{X}=\left\{S: S \subseteq[0,1]^{m}\right.  and  \left.|S|=n\right\}$,$ f:   \mathcal{X} \rightarrow \mathbb{R} $ is a continuous set function on  $\mathcal{X}$  w.r.t to Hausdorff distance  $d_{H}(\cdot, \cdot)$ , i.e.

$ \forall \epsilon>0, \exists \delta>0$ , for any $ S, S^{\prime} \in \mathcal{X}$ , if  $d_{H}\left(S, S^{\prime}\right)<\delta$ , then $ \left|f(S)-f\left(S^{\prime}\right)\right|<\epsilon$ . 

在原坐标系，$S，S^{\prime}$ 距离近，通过f变换后依然近，f为整个网络，S为点的子集(可以想象成是一把椅子由多个点组成，一张桌子由点云组成，在原坐标系距离近，经过f变换后，依然较近)

理论表明在最大集合层有足够多的神经元的情况下，即K足够大，f可以被网络任意的近似

#### 理论1：

$$
Suppose\  f: \mathcal{X} \rightarrow \mathbb{R}  \ is \ a \ continuous \ set \ function 
\ w.r.t \ Hausdorff \ distance \ d_{H}(\cdot, \cdot) 
\\ \quad \forall \epsilon>   0, \exists \ a \ continuous\ function\  h\  and\ a\ symmetric\ function\  g\left(x_{1}, \ldots, x_{n}\right)=\gamma \circ M A X,\\
such\ that\ for\ any\  S \in \mathcal{X} ,
\left|f(S)-\gamma\left(\underset{x_{i} \in S}{M A X}\left\{h\left(x_{i}\right)\right\}\right)\right|<\epsilon
$$

这个定理的证明可以在我们的补充材料中找到。关键的想法是，在最坏的情况下，网络可以通过将空间划分为同等大小的体素，学习将点云转换为体积表示。然而，在实践中，网络学会了一种更聪明的策略来探测空间，正如我们将在点函数可视化中看到的。

#### 瓶颈尺寸和稳定性

在理论和实验上，我们发现我们的网络的表现力受到最大集合层维度的强烈影响，即（1）中的K。

我们定义$u = MAX\ {h(x_i)}\ x_i∈S $为f的子网络，它将[0, 1]m中的一个点集映射为K维的矢量。

下面的定理告诉我们，输入集合中的小腐败或额外的噪声点不可能改变我们网络的输出。

#### 理论2：

$$
Theorem\ 2. \ Suppose\  \mathbf{u}: \mathcal{X} \rightarrow \mathbb{R}^{K}  \ such \ that\  \mathbf{u}=   \underset{x_{i} \in S}{\operatorname{MAX}}\left\{h\left(x_{i}\right)\right\}  \ and \ f=\gamma \circ \mathbf{u} . Then,\\
(a)  \forall S, \exists \mathcal{C}_{S}, \mathcal{N}_{S} \subseteq \mathcal{X}, f(T)=f(S)  \ if \ \mathcal{C}_{S} \subseteq T \subseteq \mathcal{N}_{S} ;\\
(b)  \left|\mathcal{C}_{S}\right| \leq K
$$

我们解释该定理的含义。(a)说，如果CS中的所有点都被保留，那么f(S)在输入腐败之前是不变的；它在NS之前也是不变的，有额外的噪声点。(b)说CS只包含有限数量的点，由（1）中的K决定。换句话说，f(S)实际上完全由一个少于或等于K元素的有限子集CS⊆S决定。因此我们称CS为S的临界点集，K为f的瓶颈维度。

结合h的连续性，这就解释了我们的模型对点扰动、腐败和额外噪声点的鲁棒性。稳健性是通过类似于机器学习模型中的稀疏性原则获得的。

直观地说，我们的网络学会了用一组稀疏的关键点来概括一个形状。在实验部分，我们看到关键点形成了一个物体的骨架。

### 实验

#### 应用

**3D目标检测**

原始点云处理

我们根据面的面积对网格面上的1024个点进行均匀采样，并将其归一化为一个单位球体。

在训练过程中，我们通过沿上轴随机旋转物体和用零平均值和0.02标准差的高斯噪声抖动每个点的位置来即时增加点云。

<img src="3D深度学习论文.assets/image-20220719132702928.png" alt="image-20220719132702928" style="zoom:67%;" />

**3D目标部分分割**

<img src="3D深度学习论文.assets/image-20220719132954813.png" alt="image-20220719132954813" style="zoom: 67%;" />

**语义分割**

为了准备训练数据，我们首先按房间分割点，然后将房间抽成面积为1米乘1米的块。

每个点由一个9维的XYZ、RGB和归一化的房间位置（从0到1）的向量表示。在训练时，我们在每个区块中随机抽取4096个点。在测试时间，我们对所有的点进行测试。我们遵循与[1]相同的协议，使用k-fold策略进行训练和测试。

<img src="3D深度学习论文.assets/image-20220719133016682.png" alt="image-20220719133016682" style="zoom:67%;" />

#### 架构设计分析

**与其他顺序无关方法比较**

***验证取max pooling （1.） 的有效性***

<img src="3D深度学习论文.assets/image-20220719130113380.png" alt="image-20220719130113380" style="zoom:67%;" />

**输入和特征转换的有效性**

***验证特征变换（3.）的有效性，感觉没什么用***

<img src="3D深度学习论文.assets/image-20220719130447674.png" alt="image-20220719130447674" style="zoom: 67%;" />

特征变换有两部分input transform和feature transform见图，根据实验和不使用相比提高2.1%

**鲁棒性测试**

![image-20220719130923172](3D深度学习论文.assets/image-20220719130923172.png)

左图：x轴：删除一定比例的点，furthest为对最原始的1024个点最远取样

中图：x轴：异常值均匀地散布在单位球体中，在空间中手动添加异常值   只用xyz坐标和加入密度

右图：x轴：高斯核的啥  在每个点独立添加高斯噪声

结论：在忽略部分点或施加噪声时，准确率依然很高

#### 网络可视化

<img src="3D深度学习论文.assets/image-20220718164517190.png" alt="image-20220718164517190" style="zoom: 67%;" />

1.原始形状  2.关键点集（下界）  3.最大上界

#### 时间空间复杂性分析

![image-20220719132518016](3D深度学习论文.assets/image-20220719132518016.png)

<img src="3D深度学习论文.assets/image-20220719132702928.png" alt="image-20220719132702928" style="zoom:67%;" />

参数量和运行速度很快，结果差不多

## PointNet++

> PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space

***针对 pointnet网络上的缺陷不能识别局部特征做改进***

***根据卷积可以提取局部特征的特性，参考滑动窗口，需要在点云上类似设计，可以想到定义一个局部的邻域球，然后进行滑动，但点云由于不连续，各区域密度不同，怎样设计结构有待解决***

***本论文的主要贡献为利用多个尺度的邻域实现鲁棒性和细节捕捉，在随机输入的协助下，该网络学会了自适应地加权在不同尺度上检测到的模式，并根据输入数据结合多尺度特征。***

### 本文方法

#### 回顾pointnet

见上面，略

#### 层次化的点集特征学习

集合抽象层由三个关键层组成。采样层、分组层和点网层。采样层从输入点中选择一个点集，它定义了局部区域的中心点。然后，分组层通过寻找中心点周围的 "相邻 "点来构建局部区域集。点网层使用迷你点网将局部区域模式编码为特征向量。如图

![image-20220714123351134](3D深度学习论文.assets/image-20220714123351134.png)

集合抽象层：(N, d+C)  -> (N1, d+C1)  ***N个点 d维坐标 C维其他特征  类似于卷积的下采样 降尺寸升通道***

- **采样层：** (N, d+C)  -> (N1, d)   

  FPS 最远点采样 **纯算法** 得到所有点的覆盖采样 作为分组中心

  <img src="3D深度学习论文.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBATWljaGFlbC5DVg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center-16582207848152.png" alt="image-20200213100836790" style="zoom: 33%;" />

- **分组层：**输入( N, d+C ), ( N1, d )(采样结果)   ->  (N1, K, d+C )

  卷积神经网络中，像素的局部区域为卷积核的大小，在点集中，点的邻域是一定半径内的区域，或是根据KNN得到最近的点 最多为K个点（可以设置）

  该层实际为根据采样层得到的中心点，每个点周围找K个点最为该点的邻域 **纯算法**

  <img src="3D深度学习论文.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBATWljaGFlbC5DVg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center-16582207997104.png" alt="image-20200213104601096" style="zoom: 50%;" />

- **点网层：** (N1, K, d+C )  -> (N1, d+C1)  

   (N1, K, d+C )  -> 卷积 (N1, K, d+C1)  -> pooling (N1, 1, d+C1)  

  提取特征，网络为pointnet

#### 非均匀采样密度下的鲁棒性特征学习

在分组局部区域和结合不同尺度的特征方面，我们提出了两类密度自适应层，如下所述。

<img src="3D深度学习论文.assets/image-20220719154428845.png" alt="image-20220719154428845" style="zoom:50%;" />

#### 语义分割的点特征传播

为了实现点云的语义分割任务，我们需要获得所有原始坐标点的特征。一种解决方案是在所有点集的Set Segmentation层中，将所有的点作为中心点进行采样，但是这会导致较高的计算成本。另一种方法是将特征从亚采样点逐层传播到原始点（图3.9）。作者采用了基于反距离权重插值（Inverse Distance Weight，IDW）和跨水平跳跃链接的分级传播策略（类似CNN的上采样）。具体实现流程如下：

![img](3D深度学习论文.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBATWljaGFlbC5DVg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center.png)

## Frustum PointNets (F-PointNet)

> Frustum PointNets for 3D Object Detection from RGB-D Data

***使用2d目标检测器辅助提出区域建议***

***大多数工作是将3d点云转化为图像或体素，然后应用网络，这种数据表示可能会掩盖数据的自然三维模式和不变性。一些论文提出直接处理点云，如pointnet，在分类和分割表现出色。但目前不清楚怎样实现目标检测，为了实现这一目标，必须解决如何实现3d的rpn，模仿2d的操作通过滑动窗口列举是可以的，但代价太高。***

***本文提出通过先按照降维原则减少搜索空间，即使用2d目标检测提取物体的三维边界框，再在每个3d框内实现目标检测实例分割***

**RGB-D图片介绍：在普通的图片上多了深度这第四个维度 （w，h，4） 可以理解为普通的rgb图加上一个代表深度的灰度图，非传统点云结构**

### 问题定义

给定RGB-D图片进行分类和定位

标签：三维边界框 （h, w, l, cx, cy, cz, θ, φ, ψ）长宽高 中心点坐标  相对于每个类别预定义的标准姿态的方向 

### 网络架构

![image-20220718133437518](3D深度学习论文.assets/image-20220718133437518.png)

![img](3D深度学习论文.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTM3MzQ4MA==,size_16,color_FFFFFF,t_70.png)

#### Frustum Proposal（截头锥体建议，见图）

深度传感器拍摄的RGB-D图像分辨率不足，远低于普通相机的RGB图像，所以作者通过成熟的2d检测提出一个二维物体区域建议，根据这个区域的方向和深度的近平面和远平面，我们就得到一个三维搜索空间的截头锥体和内部的点云。

<img src="3D深度学习论文.assets/image-20220723140037568.png" alt="image-20220723140037568" style="zoom: 67%;" />

如图a，但是截头锥体可能朝向许多不同的方向，这导致点云的位置变化，因此，通过将锥体向中心旋转，使锥体的中心和图像平面正交，这种正则化有助于提高算法的旋转不变性。

***此部分为2d的目标检测，提取定位和分类***

<img src="3D深度学习论文.assets/image-20220722211029103.png" alt="image-20220722211029103" style="zoom: 50%;" />

#### 3D Instance Segmentation

输入：（n，c）和 k  ***区域建议的点云和类别***

输出：（m，c）  ***该区域中输入该类的点***

<img src="3D深度学习论文.assets/image-20220723141300225.png" alt="image-20220723141300225" style="zoom:50%;" />

***对该区域的点做分类判断为是否为感兴趣的点  得到可能性分数 类似于RPN***

***根据分数排名选出m个点，现在这m个点的坐标是相对于蓝色中心的，为了便于进行分类和定位，转换到这些点的中心即红色中心，之后进行输出。***

#### 3D amodal bounding box estimation

<img src="3D深度学习论文.assets/image-20220723141325001.png" alt="image-20220723141325001" style="zoom:50%;" />

***第一部分已经得到了类别信息，第二部分得到了该类有哪些点，该部分是为了进行定位，也就是找到3d边界框，已经有了这个点的中心，但是由于物体不规范或是缺少部分，导致现在的中心可能不是真正的中心，所以利用T-net计算了一个中心偏移（Δcx，Δcy，Δcz，），用来找到真正的中心，经过中心偏移后，利用pointnet回归边界框（3+2NH+4NS） 3为（w，h，l）***

如第3节所述，我们通过中心（cx、cy、cz）、大小（h、w、l）和航向角θ（沿上轴线）来参数化三维边界框。我们采用“残差”方法进行箱中心估计。盒估计网络预测的中心残差与来自T网络的先前中心残差和屏蔽点的质心相结合，以恢复绝对中心（等式1）。对于箱子大小和航向角度，我们遵循了之前的工作[29，24]，并使用分类和回归公式的混合。具体来说，我们预先定义了N个S大小的模板和N个H等分角箱子。我们的模型将大小/标题（大小的N S分数，标题的N H分数）分类到那些预定义的类别，并预测每个类别的残差数（高度、宽度、长度的3×N S残差维度，标题的N H残差角度）。最后，该网络共输出3+4×N S+2×N H(3+ 2* 12+ 4* 8)个数。

3 为中心偏移回归

2* 12： 第一个12为朝向角分类，第二个12为朝向角偏移回归

此偏移以每个等分角为单位1

4* 8： 第一个8为尺寸盒分类，后三个8为w，h，l尺寸盒偏移回归

此偏移以实际物体平均大小为单位1

### 损失函数

![image-20220729193122779](3D深度学习论文.assets/image-20220729193122779.png)

![image-20220729202848629](3D深度学习论文.assets/image-20220729202848629.png)

Lseg：二分类损失

Lc1：T-net损失

Lc2：中心偏移

Lh_cls：朝向角分类损失

Lh_reg：朝向角偏移回归损失

Ls_cls：尺寸盒分类损失

Ls_reg：尺寸盒三维偏移回归损失

rLcorner：正则项

角点损失：类似于边界框的四个坐标的回归损失 这里计算八个点的偏移 pk*指的是真实框的八个点，pk**是指角度转180度后的八个点，取最小，乘系数

## VoxelNet

> VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection

***点云转体素，主要有三个模块：***

- ***特征学习网络***
- ***卷积中间层***
- ***区域建议网络***

### 特征学习网络

![image-20220731125326115](3D深度学习论文.assets/image-20220731125326115.png)

- **体素划分：**将三维空间（D, H, W）分为等距体素 size $v_D,v_H,v_W$

- **分组：**每个体素空间为一组，如图共4组，但这四组内的点云个数互不相同，有多有少还有空
- **随机采样：**为了减少计算量和平均点云密度，从超过T个点的体素中随机取T个点，不够就全取，没有的不要

- **堆叠VFE层：**在每个体素中提取局部特征

  <img src="3D深度学习论文.assets/image-20220731130736977.png" alt="image-20220731130736977" style="zoom: 33%;" />

  

​	假设体素中有t个点 t≤T 得到输入(4, t) (x, y, z, r)经过全连接网络 得到 (c1, t) 再maxpooling  (c1) 拼接 (2c1, t)

​	n层VFE之后得到(2cn, t) maxpooling (2cn)

​	这样每个体素都提取到了自己的特征，没有点云的体素为空，如图

​	为了使用gpu优化同时训练而不是每一个体素跑一次，设计了如下方法：

<img src="3D深度学习论文.assets/image-20220731155824884.png" alt="image-20220731155824884" style="zoom: 50%;" />

初始化了一个K×T×7的张量来存储数据，K为非空体素个数，T为每个体素的最多点云数，7为xyzr和xyz距体素内点云中心偏移，t不够时补0凑T维。预先建立一个哈希表，用来存储每个体素的点，体素坐标为键

输入(7, T, K) 输出(C, 1, K)

根据索引找到对应位置，重新组织为密集体素网格

这样就得到了（C, D, H, W）的特征图

### 卷积中间层

输入（C, D, H, W）128 × 10 × 400 × 352  开卷

Conv3D(128, 64, 3,(2,1,1), (1,1,1))

Conv3D(64, 64, 3, (1,1,1), (0,1,1))

Conv3D(64, 64, 3, (2,1,1), (1,1,1))

输出：64 × 2 × 400 × 352

reshape：128 × 400 × 352

### 区域建议网络

![image-20220731152435983](3D深度学习论文.assets/image-20220731152435983.png)



输入：（128, H, W）128×400×352

输出：（2, H/2, W/2）和（2*7, H/2, W/2）

第一个2为是否物体锚框的分类，类似于rpn判断前景背景

后面的2*7是2个锚框，7维的坐标

### 损失函数

![image-20220731153336328](3D深度学习论文.assets/image-20220731153336328.png)

![image-20220731153346059](3D深度学习论文.assets/image-20220731153346059.png)

### 训练细节

二分类判断标准：

​	汽车：鸟瞰图最高iou或iou>0.6为正样本 <0.45为负样本 中间不考虑α=1.5 β=1

​	行人和单车：iou>0.5为正样本 <0.35

关于输入坐标范围，体素大小，T的选取卷积核步长，锚框尺寸和数量均不同

两层VFEVFE-1(7, 32) and VFE-2(32, 128)  128 × 10 × 400 × 352

|            | 输入坐标范围 m                  | 体素大小D,H,W | T    | 锚框尺寸lwh              | 锚框数量  |
| ---------- | ------------------------------- | ------------- | ---- | ------------------------ | --------- |
| 汽车       | [−3, 1] × [−40, 40] × [0, 70.4] | 10, 400, 352  | 35   | 3.9, 1.6, 1.56 z = −1.0  | 2 0°和90° |
| 行人和单车 | [−3, 1] × [−20, 20] × [0, 48]   | 10,200,240    | 45   | 1.76, 0.6, 1.73 z = −0.6 | 2 0°和90° |

这意思是汽车和行人单车分开检测吗

## PointPillars

> PointPillars: Fast Encoders for Object Detection from Point Clouds

***以点云为输入，实现汽车、行人、单车的3d目标检测，包括三个主要阶段：***

- ***将点云转化为稀疏伪图像***
- ***2d卷积主干进行特征提取***
- ***检测头，回归3d边界框***

![image-20220730154947842](3D深度学习论文.assets/image-20220730154947842.png)

### 点云转化为伪图像

为了使用2d卷积，将点云转换为图像处理

<img src="3D深度学习论文.assets/image-20220730155013761.png" alt="image-20220730155013761" style="zoom: 50%;" />

按点云数据的XY轴划分成一个个网格假设w* h=P个 ，每个网格为一个pillar，每个pillar里取N个点（取样策略为够N个就随机取N个，不够就全取补0凑N个），每个点有$(x,y,z,r)$四个维度，r为反射强度，现加入$(x_c,y_c,z_c,x_p,y_p)$，c下标为该点到当前pillar里所有点坐标平均值的差值，p下标为该点到当前pillar中心的偏移，共9个维度。现在我们就有了(9, P, N)的点云。

接下来对点云数据进行特征提取，再按照pillar所在维度进行max pooling操作，为了获得伪图像再展成图片格式

即(9, P, N) -> (C, P, N) Maxpooling ->  (C, P) -> (C, H, W) 

### 2d特征提取主干网络

![image-20220730161320225](3D深度学习论文.assets/image-20220730161320225.png)

使用了多尺度，见图，每层上采样到（2C, H/2, W/2），再进行拼接。

### 3d检测头

与2d检测头作分类（k）和回归框（x，y，w，h）相比

3d检测头作分类（k）和回归框（x，y，z，w，h，l，θ），使用了两个锚框，结果为k+ 2*7

### 损失函数

<img src="3D深度学习论文.assets/image-20220730205547739.png" alt="image-20220730205547739" style="zoom: 67%;" />

 <img src="3D深度学习论文.assets/image-20220730205733188.png" alt="image-20220730205733188" style="zoom: 67%;" />

分类损失 focal loss

![image-20220730205750740](3D深度学习论文.assets/image-20220730205750740.png)

<img src="3D深度学习论文.assets/image-20220730205846471.png" alt="image-20220730205846471" style="zoom: 50%;" />

由于角度定位损失无法区分翻转的盒子，我们在离散化方向[28]上使用softmax分类损失，Ldir，这使网络能够学习方向，二分类是否翻转

## Votenet

> Deep Hough Voting for 3D Object Detection in Point Clouds

霍夫变换将点样本中检测简单模式的问题转化为在参数空间中检测峰值。广义霍夫变换扩展了这种技术，将图像patch作为复杂物体存在的指示器。

简单霍夫变换：点 -> 线，圆

广义霍夫变换：由线组成的物体边缘 -> 物体

https://zhuanlan.zhihu.com/p/203292567

***3d目标检测主要问题是找到物体中心及表示物体的点云，然后回归出边界框，本论文就是根据物体表面的点云，通过投票的方式投出物体中心，从而根据中心找到谁投的***

### 深度学习中的霍夫投票

传统的霍夫投票2d检测器包含一个离线步骤和一个在线步骤，首先，给定一组带注释的对象边界框的图像，使用存储的图像补丁(或其特征)及其到相应对象中心的偏移量之间的映射来构建一个代码本。在推理时，从图像中选择兴趣点，提取兴趣点周围的斑块。然后将这些补丁与代码本中的补丁进行比较，以检索偏移量并计算选票。由于对象补丁会倾向于投票表示同意，所以在对象中心附近会形成簇。最后，通过跟踪聚类投票返回其生成的patch来获取对象边界。

### 网络架构

![image-20220802134424491](3D深度学习论文.assets/image-20220802134424491.png)

#### 生成投票种子点 图1-2

从N个点中生成M张投票点并学习特征 (N, 3) -> (M, 3+C)

主干网络：pointnet++ 四层sa和两层fc

sa：通过最远点采样，逐层减少点数量，并提取局部特征

fc：特征提取

#### 投票 图2-3

(M, 3+C) -> (M, 3+C)

具体算法：

(M, 3+C)拆分坐标和特征 (M, 3)  (M, C) 

 (M, C) -> (M, (3+C)* factor) resize-> (M, factor, 3+C)

factor为投票系数1，每个点产生的投票数

3为坐标偏移 C为特征偏移

M个点扩充factor倍后分别加上偏移，就得到了(M*factor, 3)个点

对应M个点扩充factor倍的原特征也加上特征偏移得到(M* factor, C)

#### 聚类 图3-4

这一步是把各个中心和属于它们的点找到

和pointnet++的一样，得到K个中心点

采样128个点，分组每组16个点

全连接 max

最终得到 (K, 3+C)

#### 生成建议 图4-5

三层线性 最后得到 (K, 2+3+12* 2+10* 4+cls)

2：是否是物体分类

3：中心偏移回归

12：朝向分类和偏移回归

10：尺寸分类和偏移回归

cls：类别分类

### 损失函数

和前面一样[点击跳转](# 损失函数)

## DSGN

> DSGN: Deep Stereo Geometry Network for 3D Object Detection

***基于双目相机实现深度估计，接近直接采用激光雷达的方法***

***现在分成两种解决方案：***

- ***两个子任务：深度预测+目标检测***
- ***两个阶段：生成中间点云+3d目标检测（此方法由于中间转换不可微，需分成两步）***

***我们的方案：通过2d预测3d体素+目标检测（可微，端到端）***

### 网络架构

![image-20220805143426669](3D深度学习论文.assets/image-20220805143426669.png)

### 2D Feature Extraction

特征提取主干来自PSMNet，并做出了修改

得到结果(32, H/4, W/4)

### Constructing PSV

将双目的特征图通过计算差异图得到深度维（应该是算法）(64,D/4, H/4, W/4)

经过两层卷积再接Hourglass网络进行特征融合得到(64,D/4, H/4, W/4)

### 3DGV

将上步得到的结果和2d得到的特征图进行拼接

这里的3d特征图为体素特征图，长宽高为（WV , HV , DV）

### 3D  Object Detection

通过将高度为下采样得到鸟瞰图，在经过2d hourglass后回归结果

这篇文章没看到什么亮点，改进的Stereo R-CNN，建议直接看Stereo R-CNN

![image-20220806143334289](3D深度学习论文.assets/image-20220806143334289.png)

## MonoPair

> MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships

### 网络架构

![image-20220806101244657](3D深度学习论文.assets/image-20220806101244657.png)

输入2d单目图像，经过主干网络centernet后生成(W, H, 64)特征图，有11个预测分支，三个用于2d对象检测，六个用于3d对象检测，两个用于成对约束预测。

#### 2D目标检测

heatmap：（W, H, c）进行关键点定位和分类  c=3 三个类别

dimension：边界框大小回归

offest：世界坐标偏移回归，就是2d中心偏移

<img src="3D深度学习论文.assets/image-20220806143803697.png" alt="image-20220806143803697" style="zoom:50%;" />

#### 3D目标检测

depth：深度，考虑到深度回归预测困难，采用预测逆深度$z=1/\sigma(\hat{z})-1$

depth σ：深度的不确定性，就是概率或者说置信度

offset：3d中心偏移回归 我们预测的值（ug，vg，z）为相机坐标系，需转为世界坐标系

<img src="3D深度学习论文.assets/image-20220806143743096.png" alt="image-20220806143743096" style="zoom: 33%;" />

offset σ：中心偏移概率

dimension：长宽高直接回归

rotation：八个方向分类

#### 成对空间约束（重点）

distance：相邻两物体距离

distance σ：置信度

想法：**估计在2D检测中得到的keypoints中两两临近的点构成的空间约束**

不是所有点都构成约束，如图，只有相邻的点之间才有约束。

以两个kepoints之间的距离作为直径去画圆，当圆中有包含有其他object点时就直接忽略该约束。

<img src="3D深度学习论文.assets/image-20220806150352165.png" alt="image-20220806150352165" style="zoom:50%;" />

<img src="3D深度学习论文.assets/image-20220806150457773.png" alt="image-20220806150457773" style="zoom:50%;" />

通过上面两个分支，我们假设知道了两个关键点以及它的2d坐标$c^b_i,c^b_j$，3d坐标$c^w_i,c^w_j$

对于2d，约束可以表示为$p_{ij}^b=(c^b_i+c^b_j)/2$

在3d相机坐标系下的距离为$K_{ij}^w=(c^w_i-c^w_j)$

然后做变换转为世界坐标系

![image-20220806152724378](3D深度学习论文.assets/image-20220806152724378.png)

<img src="3D深度学习论文.assets/image-20220806152735641.png" alt="image-20220806152735641" style="zoom:50%;" />

即为空间约束

#### 空间约束优化

最终的空间约束优化可以变换成一个非线性最小二乘问题

<img src="3D深度学习论文.assets/image-20220806152852284.png" alt="image-20220806152852284" style="zoom:50%;" />